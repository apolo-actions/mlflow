kind: live
name: Neu.ro MLflow server
author: Neu.ro MLOps team
descr: Integrate and run MLFlow server in your project.

inputs:
  mode:
    default: "server"
    descr: |
      Allowed values are "server" or "ui". This is a mode of operation of MLFlow server.
      In "server" mode, the MLFLow server should run while MLFlow Client is connected to it.
      This mode is also required to run in-job ML models deployments.
  artifacts_destination:
    descr: |
      Place, where to store MLFlow artifacts (such as model dumps).
      Beware, this path should also be accessible from the training job
      You might also use a platform storage as a backend.
      In this case use a local path for artifact store:
        1. set this input equal to the `mount path` of needed volume,
        2. add its read-write reference to the `inputs.volumes` list
      Docs: https://mlflow.org/docs/latest/tracking.html#artifact-stores
  backend_store_uri:
    default: ""
    descr: |
      URI to the storage, which should be used to dump experiments metadata and model registry.
      Works only in "server" mode.
      Examples:
        Empty string: in this case the model registry functionality will not work
        Postgress DB: postgresql://postgres:password@job-1231-123123-123.platform-jobs:5432
        SQLite DB: sqlite:///some/path/mlflow.db - you cannot use the platform storage if it's running in Azure, but could use disk
        regular file: /path/to/store - in this case "MLFlow registered models" will not work.
      Docs: https://mlflow.org/docs/latest/tracking.html#backend-stores
  volumes:
    default: "[]"
    descr: Additional volumes for the server job
  envs:
    default: "{}"
    descr: Additional environment variables for the server job
  http_auth:
    default: ""
    descr: |
      Whether to enable Neu.ro platform-based authentication or not.
      If disabled (by default) - your MLFlow server will not be protected by Neu.ro SSO.
  life_span:
    default: "10d"
    descr: |
      How long should the MLFlow server job run
  port:
    default: "5000"
    descr: |
      Port, which the MLFlow server listens will listen to and the platform exposes.
  job_name:
    default: ""
    descr: |
      The name of MLFlow server job.
      Use it to generate a predictable job hostname.
  preset:
    default: ""
    descr: |
      The amount of resources to use, while running the server.
  extra_params:
    default: ""
    descr: |
      Extra parameters for `mlflow server` command invocation.

job:
  image: ghcr.io/neuro-inc/mlflow:v1.28.0
  name: ${{ inputs.job_name }}
  preset: ${{ inputs.preset }}
  http_port: ${{ inputs.port }}
  http_auth: ${{ inputs.http_auth }}
  life_span: ${{ inputs.life_span }}
  # If workdir is not set, mlflow server will not grasp artifacts without backend_store_uri
  workdir: ${{ inputs.artifacts_destination if inputs.artifacts_destination[0] == "/" else "/" }}
  detach: True
  browse: True
  volumes: "${{ from_json(inputs.volumes) }}"
  env: "${{ from_json(inputs.envs) }}"
  cmd: |
    ${{ "ui" if lower(inputs.mode) == "ui" else "server" }}
      --host 0.0.0.0 --port ${{ inputs.port }}
      ${{ ("--backend-store-uri=" + inputs.backend_store_uri ) if inputs.backend_store_uri else "" }}
      --artifacts-destination=${{ inputs.artifacts_destination }} 
      ${{ "--serve-artifacts" if lower(inputs.mode) == "server" else "" }}
      ${{ inputs.extra_params }}
